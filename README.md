# ğŸ›’ Sales Forecasting Research
### Feature Engineering vs Deep Learning

This is my final assessment for **CSO7013 Machine Learning** at St Mary's University. The project investigates whether traditional machine learning with hand-crafted features can match deep learning performance for retail sales forecasting.

---

## ğŸ”¬ Research Question

> **Can lightweight feature engineering with gradient boosting match LSTM deep learning performance while training significantly faster?**

I hypothesised that domain-informed features (lags, rolling statistics, calendar encodings) can capture sales patterns as effectively as an LSTM's learned representations â€” but with a fraction of the computational cost.

---

## ğŸ“Š Dataset

I used the **UCI Online Retail Dataset** â€” a real-world transactional dataset containing 541,909 purchases from a UK-based online retailer (Dec 2010 â€“ Dec 2011).

| Attribute | Value |
|-----------|-------|
| Source | UCI Machine Learning Repository |
| DOI | 10.24432/C5BW33 |
| Licence | CC BY 4.0 |
| Transactions | 541,909 |

---

## ğŸ§ª Methodology

I compared two approaches on the same data:

### 1ï¸âƒ£ LightGBM + Feature Engineering
- **20 hand-crafted features** across 5 groups:
  - **Temporal**: day_of_week, month, is_weekend, cyclical encodings
  - **Lag**: sales from 7, 14, 21, 28 days ago
  - **Rolling**: 7-day and 14-day moving averages, standard deviation
  - **External**: holidays, promotions
  - **Encoded**: target-encoded store and product family

### 2ï¸âƒ£ LSTM Neural Network
- 2-layer LSTM with 32 hidden units
- 28-day input sequences
- Learns patterns directly from raw data

### Baseline
- **Seasonal NaÃ¯ve**: predicts sales = last week's sales (lag-7)

---

## ğŸ“ˆ Key Findings

My experiments showed that **feature engineering beats deep learning** for this task:

| Model | RMSLE â†“ | Training Time |
|-------|---------|---------------|
| Seasonal NaÃ¯ve | 1.04 | <1s |
| **LightGBM** | **0.42** â­ | **11s** |
| LSTM | 0.48 | 55s |

- LightGBM achieved **12.5% lower error** than LSTM
- LightGBM trained **5x faster**
- Lag features contributed most to performance (ablation study confirmed)

---

## ğŸ’¡ Conclusion

For structured time-series forecasting like retail sales, **carefully engineered features combined with gradient boosting can outperform deep learning** while being faster, more interpretable, and easier to deploy.

This aligns with recent research (Grinsztajn et al., 2022) showing tree-based models often beat neural networks on tabular data.

---

## ğŸ“ Project Structure

```
â”œâ”€â”€ main.py                 # Run experiment
â”œâ”€â”€ downloadData.py         # Fetch UCI dataset
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ dataLoader.py       # Load & preprocess data
â”‚   â”œâ”€â”€ featureEngineer.py  # Create 20 features
â”‚   â”œâ”€â”€ lightgbmModel.py    # LightGBM forecaster
â”‚   â”œâ”€â”€ lstmModel.py        # LSTM neural network
â”‚   â”œâ”€â”€ evaluator.py        # RMSLE, MAE metrics
â”‚   â””â”€â”€ visualizer.py       # Generate figures
â””â”€â”€ experiments/
    â”œâ”€â”€ figures/            # Charts for paper
    â””â”€â”€ results/            # JSON metrics
```

---

## ğŸ“š References

- Chen, D. (2015). Online Retail [Dataset]. UCI ML Repository. https://doi.org/10.24432/C5BW33
- Ke, G. et al. (2017). LightGBM: A Highly Efficient Gradient Boosting Decision Tree. NeurIPS.
- Grinsztajn, L. et al. (2022). Why do tree-based models still outperform deep learning on tabular data? NeurIPS.

---

## ğŸ“„ Licence

MIT License â€” see [LICENSE](LICENSE)

---

**Student ID:** 2517238  
**Module:** CSO7013 Machine Learning  
**Institution:** St Mary's University
